## Language Modeling:
- [Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning](https://aclanthology.org/2022.coling-1.382/)
- [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
- [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://arxiv.org/abs/2205.06266)
- [MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer](https://aclanthology.org/2020.emnlp-main.617/)
- [SwissBERT: The Multilingual Language Model for Switzerland](https://arxiv.org/abs/2303.13310)
- [Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages](https://aclanthology.org/2021.mrl-1.11/)
- [Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training](https://arxiv.org/abs/2212.10503)
- [Bleu: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040/)
- [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [Investigating the Existence of "Secret Language" in Language Models](https://arxiv.org/abs/2307.12507v1)
- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
- [Specializing Multilingual Language Models: An Empirical Study](https://aclanthology.org/2021.mrl-1.5/)
- [Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages](https://arxiv.org/abs/2305.12182)
- [Unsupervised Neural Machine Translation](https://arxiv.org/abs/1710.11041)
- [On the Cross-lingual Transferability of Monolingual Representations](https://arxiv.org/abs/1910.11856)
- [Improving Language Plasticity via Pretraining with Active Forgetting](https://arxiv.org/abs/2307.01163)
- [MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition](https://aclanthology.org/2022.emnlp-main.298/)
- [Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains](https://arxiv.org/abs/2106.13474)
- [BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting](https://arxiv.org/abs/2212.09535)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
- [Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of Yorùbá and Twi](https://aclanthology.org/2020.lrec-1.335/)
- [PolyLM: An Open Source Polyglot Large Language Model](https://arxiv.org/abs/2307.06018)
- [AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages](https://arxiv.org/abs/2305.06897)
- [BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages](https://arxiv.org/abs/2305.18098)
- [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)
- [Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation](https://arxiv.org/abs/2203.09435)
- [Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation](https://arxiv.org/abs/2205.12647)
- [A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models](https://arxiv.org/abs/2309.11674)
- [Extrapolating Large Language Models to Non-English by Aligning Languages](https://arxiv.org/abs/2308.04948)

## Language Embeddings:
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://aclanthology.org/2021.emnlp-main.800/)
- [Improving Pre-Trained Multilingual Models with Vocabulary Expansion](https://arxiv.org/abs/1909.12440)
- [Learning bilingual word embeddings with (almost) no bilingual data](https://aclanthology.org/P17-1042/)
- [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821)
- [Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages](https://arxiv.org/abs/2309.04679)
- [Word Translation Without Parallel Data](https://arxiv.org/abs/1710.04087)
- [From English To Foreign Languages: Transferring Pre-trained Language Models](https://arxiv.org/abs/2002.07306)
- [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://arxiv.org/abs/2112.06598)
- [FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language](https://arxiv.org/abs/2305.14481)
- [Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation](https://arxiv.org/abs/2310.03477)
- [GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost](https://arxiv.org/abs/2211.06993)

## Tokenization:
- [Language Model Tokenizers Introduce Unfairness Between Languages](https://arxiv.org/abs/2305.15425)
- [Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages](https://arxiv.org/abs/2305.17179)
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)
- [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://aclanthology.org/2020.findings-emnlp.414/)
- [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)

## Parameter Efficient Finetuning:
- [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [TOAST: Transfer Learning via Attention Steering](https://arxiv.org/abs/2305.15542v2)
- [Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding](https://arxiv.org/abs/2307.07880)
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
- [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)
- [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

## Neural Information Retrieval:
- [Passage Re-ranking with BERT](https://arxiv.org/abs/1901.04085)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval](https://arxiv.org/abs/2101.00436)
- [Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP](https://arxiv.org/abs/2212.14024)

## Deep Learning:
- [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
- [Causal Reinforcement Learning: A Survey](https://arxiv.org/abs/2307.01452v1)
- [Loss of Plasticity in Deep Continual Learning](https://arxiv.org/abs/2306.13812)

## Prompting + Agent LLMs + other:
- [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)
- [Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)
- [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/abs/2303.06865)
- [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
- [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)
- [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)
- [Lost in the Middle: How Language Models Use Long Context](https://arxiv.org/abs/2302.00093)
